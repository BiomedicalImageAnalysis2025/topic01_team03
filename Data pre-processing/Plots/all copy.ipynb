{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb8c2f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 331\u001b[39m\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m ! Kein Ground-Truth für \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_file.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ! überspringe …\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    329\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     score = \u001b[43mprocess_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     dice_scores.append(score)\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# Vector containing all dice scores\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 281\u001b[39m, in \u001b[36mprocess_single\u001b[39m\u001b[34m(img_path, gt_path)\u001b[39m\n\u001b[32m    278\u001b[39m img_meanfiltered = mean_filter(img_scaled)\n\u001b[32m    280\u001b[39m \u001b[38;5;66;03m# Wiener filter, background estimation and removal\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m bg  = \u001b[43mlocal_wiener_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_meanfiltered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m img_filtered = img_scaled - bg\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Gamma transformation\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mlocal_wiener_filter\u001b[39m\u001b[34m(image, window_size, noise_variance)\u001b[39m\n\u001b[32m     36\u001b[39m window = img_padded[i:i+window_size, j:j+window_size]\n\u001b[32m     37\u001b[39m local_mean = window.mean()\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m local_var = \u001b[43mwindow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Compute Wiener filter response\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_var > noise_variance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/numpy/_core/_methods.py:171\u001b[39m, in \u001b[36m_var\u001b[39m\u001b[34m(a, axis, dtype, out, ddof, keepdims, where, mean)\u001b[39m\n\u001b[32m    166\u001b[39m     arrmean = mean\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Compute the mean.\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# Note that if dtype is not of inexact type then arraymean will\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;66;03m# not be either.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     arrmean = \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# The shape of rcount has to match arrmean to not change the shape of\u001b[39;00m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# out in broadcasting. Otherwise, it cannot be stored back to arrmean.\u001b[39;00m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m rcount.ndim == \u001b[32m0\u001b[39m:\n\u001b[32m    175\u001b[39m         \u001b[38;5;66;03m# fast-path for default case when where is True\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from  matplotlib.image import imread\n",
    "from pathlib import Path\n",
    "from skimage.io import imread\n",
    "from PIL import Image\n",
    "from typing import Union, Tuple\n",
    "\n",
    "#------ALL FUNCTIONS NEEDED------\n",
    "\n",
    "# Wiener filter\n",
    "def local_wiener_filter(image, window_size=201, noise_variance=None):\n",
    "    \"\"\"\n",
    "    Apply a local adaptive Wiener filter to a grayscale image.\n",
    "\n",
    "    Parameters:\n",
    "    - image: 2D numpy array of the grayscale image.\n",
    "    - window_size: size of the square window (odd integer).\n",
    "    - noise_variance: estimated variance of the noise; if None, estimate globally.\n",
    "\n",
    "    Returns:\n",
    "    - filtered: the Wiener-filtered image as a 2D numpy array.\n",
    "    \"\"\"\n",
    "    # Pad the image to handle borders\n",
    "    pad = window_size // 2\n",
    "    img_padded = np.pad(image, pad, mode='reflect')\n",
    "\n",
    "    # Estimate global noise variance if not provided\n",
    "    if noise_variance is None:\n",
    "        noise_variance = np.var(image - np.mean(image))\n",
    "\n",
    "    filtered = np.zeros_like(image)\n",
    "    # Slide window over image\n",
    "    for i in range(filtered.shape[0]):\n",
    "        for j in range(filtered.shape[1]):\n",
    "            window = img_padded[i:i+window_size, j:j+window_size]\n",
    "            local_mean = window.mean()\n",
    "            local_var = window.var()\n",
    "            # Compute Wiener filter response\n",
    "            if local_var > noise_variance:\n",
    "                filtered[i, j] = local_mean + (local_var - noise_variance) / local_var * (image[i, j] - local_mean)\n",
    "            else:\n",
    "                filtered[i, j] = local_mean\n",
    "\n",
    "    f = filtered\n",
    "    f_norm = (f - f.min()) / (f.max() - f.min())\n",
    "    filtered = f_norm\n",
    "    return filtered\n",
    "\n",
    "# Gamma transformation\n",
    "def gammatransformation(image, gamma=None):\n",
    "   \"\"\"\n",
    "    Every pixel value p is transformed by p^gamma.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameter1 : gray level image\n",
    "        A 2D array containing all the gray levels of each pixel.\n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    new image as 2D array.\n",
    "       Each pixel value of the transformed image is result of p^gamma.\n",
    "       Result is in the range 0-1\n",
    "    \"\"\"\n",
    "   \n",
    "   \n",
    "   # Check if there are negative values in the array and if so, shift into positive range\n",
    "   if np.any(image<0):\n",
    "       image = image + abs(np.min(image))\n",
    "\n",
    "   # Scale to 0-1\n",
    "   if image.min() != 0 or image.max() != 1:\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    \n",
    "   # Define gamma based on mean illumination\n",
    "   if gamma is None:\n",
    "     if np.mean(image) > 0.5:\n",
    "        gamma = 1.5\n",
    "     else:\n",
    "       gamma = 0.1\n",
    "\n",
    "\n",
    "   # Perform gamma transformation\n",
    "   img_gamma = np.power(image, gamma)\n",
    "\n",
    "   # Scale back to 0-255 8 bit\n",
    "   img_gamma = (img_gamma * 255).astype(np.uint8)\n",
    "\n",
    "   return img_gamma\n",
    "\n",
    "# Histogram equalization\n",
    "def histogramequalization(image):\n",
    "   \"\"\"\n",
    "    Spreads the intensity values to the full range of 0-255.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parameter1 : gray level image\n",
    "        A 2D array containing all the gray levels of each pixel.\n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    new image as 2D array.\n",
    "       New image uses the full range of 0-255\n",
    "    \"\"\"\n",
    "   if np.max(image) == 255 and np.min(image) == 0:\n",
    "      image_8bit = (image).astype(np.uint8)\n",
    "   else:\n",
    "      image_8bit = (((image - image.min()) / (image.max() - image.min())) * 255).astype(np.uint8)\n",
    "\n",
    "   hist, bins = np.histogram(image_8bit.flatten(), bins=256, range=[0, 256])\n",
    "  \n",
    "   cdf = hist.cumsum()\n",
    "  \n",
    "   cdf_normalized = cdf * 255 / cdf[-1] \n",
    "\n",
    "   image_eq = cdf_normalized[image_8bit]\n",
    "\n",
    "   image_eq = image_eq.astype(np.uint8)\n",
    "\n",
    "   return image_eq\n",
    "\n",
    "# Mean filter\n",
    "def mean_filter(image, kernel_size = 15) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply a mean filter (box blur) to a 2D grayscale image using only NumPy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        2D array of the grayscale image.\n",
    "    kernel_size : int\n",
    "        Size of the (square) kernel; must be odd.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    filtered : np.ndarray\n",
    "        The blurred image, same shape as input.\n",
    "    \"\"\"\n",
    "    if kernel_size % 2 == 0:\n",
    "        raise ValueError(\"kernel_size must be odd.\")\n",
    "    pad = kernel_size // 2\n",
    "    # Spiegele die Ränder, damit wir auch am Rand filtern können\n",
    "    img_padded = np.pad(image, pad, mode='reflect')\n",
    "    H, W = image.shape\n",
    "    filtered = np.empty_like(image, dtype=float)\n",
    "\n",
    "    # Summen-Integralbild zur schnellen Fenster-Summen\n",
    "    # Integralbild hat eine zusätzliche Null-Zeile/-Spalte vorne\n",
    "    integral = np.cumsum(np.cumsum(img_padded, axis=0), axis=1)\n",
    "    # Schleife über alle Pixel\n",
    "    for i in range(H):\n",
    "     for j in range(W):\n",
    "      window = img_padded[i:i+kernel_size, j:j+kernel_size]\n",
    "      filtered[i,j] = window.sum() / (kernel_size**2)\n",
    "    return filtered\n",
    "\n",
    "# creating histogram for Otsu threshholding  \n",
    "def custom_histogram(image: np.ndarray, nbins: int = 256) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Computes the histogram and corresponding bin centers of a grayscale image,\n",
    "    replicating the behavior of skimage.exposure.histogram, including normalization\n",
    "    to the [0, 255] range. This ensures consistent behavior with Otsu implementations\n",
    "    that assume 8-bit images.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Input image as a 2D array of grayscale values.\n",
    "        nbins (int): Number of bins for the histogram (default: 256).\n",
    "\n",
    "    Returns:\n",
    "        hist (np.ndarray): Array of histogram frequencies for each bin.\n",
    "        bin_centers (np.ndarray): Array of bin center values.\n",
    "    \"\"\"\n",
    "    # Determine the minimum and maximum pixel intensity in the image\n",
    "    img_min, img_max = image.min(), image.max()\n",
    "\n",
    "    # Normalize the image intensities to the range [0, 255], as in skimage\n",
    "    image_scaled = (image - img_min) / (img_max - img_min) * 255\n",
    "\n",
    "    # Compute the histogram of the scaled image within [0, 255]\n",
    "    hist, bin_edges = np.histogram(\n",
    "        image_scaled.ravel(),\n",
    "        bins=nbins,\n",
    "        range=(0, 255)\n",
    "    )\n",
    "\n",
    "    # Compute bin centers as the average of adjacent bin edges\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "    return hist, bin_centers\n",
    "\n",
    "# Otsu thresholding\n",
    "def apply_global_otsu(image: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes the global Otsu threshold of an input grayscale image in a way that matches\n",
    "    the behavior of skimage.filters.threshold_otsu, including histogram scaling and\n",
    "    threshold rescaling back to the original intensity range.\n",
    "\n",
    "    This function enables nearly identical thresholding results to skimage's implementation,\n",
    "    even on images with floating-point or non-8-bit integer data.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Input image as a 2D array of grayscale values.\n",
    "\n",
    "    Returns:\n",
    "        threshold_original (float): Computed Otsu threshold mapped back to the original image range.\n",
    "    \"\"\"\n",
    "    # Compute histogram and bin centers consistent with skimage\n",
    "    hist, bin_centers = custom_histogram(image, nbins=256)\n",
    "    hist = hist.astype(np.float64)\n",
    "\n",
    "    # Normalize histogram to obtain probability distribution p(k)\n",
    "    p = hist / hist.sum()\n",
    "\n",
    "    # Compute cumulative sums of class probabilities ω0 and ω1\n",
    "    omega0 = np.cumsum(p)                           # Class probabilities for background\n",
    "    omega1 = np.cumsum(p[::-1])[::-1]               # Class probabilities for foreground\n",
    "\n",
    "    # Compute cumulative sums of class means μ0 and μ1\n",
    "    mu0 = np.cumsum(p * bin_centers)                # Class means for background\n",
    "    mu1 = np.cumsum((p * bin_centers)[::-1])[::-1]  # Class means for foreground\n",
    "\n",
    "    # Compute between-class variance σ_b^2 for each possible threshold\n",
    "    sigma_b_squared = (omega0[:-1] * omega1[1:] * (mu0[:-1] / omega0[:-1] - mu1[1:] / omega1[1:])**2)\n",
    "\n",
    "    # Find the threshold index t maximizing σ_b^2\n",
    "    t_idx = np.argmax(sigma_b_squared)\n",
    "    t_scaled = bin_centers[t_idx]\n",
    "\n",
    "    # Rescale threshold t back to original image intensity range\n",
    "    img_min, img_max = image.min(), image.max()\n",
    "    t_original = t_scaled / 255 * (img_max - img_min) + img_min\n",
    "\n",
    "    return (image > t_original).astype(np.uint8)\n",
    "\n",
    "# Dice score\n",
    "def dice_score(otsu_img, otsu_gt):\n",
    "\n",
    "    # control if the Pictures have the same Size\n",
    "    if len(otsu_img) != len(otsu_gt):\n",
    "       if len(otsu_img) > len(otsu_gt):\n",
    "         otsu_img = otsu_img[1:len(otsu_gt)]\n",
    "       else:\n",
    "        otsu_gt = otsu_gt[1:len(otsu_img)]\n",
    "\n",
    "\n",
    "    # defining the variables for the Dice Score equation\n",
    "    positive_overlap = 0\n",
    "    sum_img = 0\n",
    "    sum_gt = 0\n",
    "\n",
    "    for t, p in zip(otsu_img, otsu_gt):\n",
    "        if t == 1:\n",
    "            sum_img += 1\n",
    "        if p == 1:\n",
    "            sum_gt += 1\n",
    "        if t == 1 and p == 1:\n",
    "            positive_overlap += 1\n",
    "\n",
    "    if sum_img + sum_gt == 0:\n",
    "        return 1.0\n",
    "\n",
    "    return 2 * positive_overlap / (sum_img + sum_gt)\n",
    "\n",
    "# Process single image and its groundtruth\n",
    "def process_single(img_path: Path, gt_path: Path) -> float:\n",
    "    \"\"\" \n",
    "    Reads one image and corresponding groundtruth, proccesses, segments and computes dice score for one image.\n",
    "    \"\"\"\n",
    "    # Reads image and reads, binarizes groundtruth\n",
    "    img = imread(img_path, as_gray=True)\n",
    "    img_scaled  = (img / img.max() * 255).astype(np.uint8)\n",
    "    gt = imread(gt_path, as_gray=True)\n",
    "    gt  = 1 - ((gt / gt.max() * 255).astype(np.uint8) == 0)         \n",
    "\n",
    "    # Mean filter\n",
    "    img_meanfiltered = mean_filter(img_scaled)\n",
    "\n",
    "    # Wiener filter, background estimation and removal\n",
    "    bg  = local_wiener_filter(img_meanfiltered)\n",
    "    img_filtered = img_scaled - bg\n",
    "\n",
    "    # Gamma transformation\n",
    "    img_gamma = gammatransformation(img_filtered, gamma=0.6)\n",
    "\n",
    "    # Hist-Equalization\n",
    "    img_eq = histogramequalization(img_gamma)\n",
    "\n",
    "    # 1nd Otsu-thresholding + Meanfilter + 2nd Otsu thresholding \n",
    "    \n",
    "    # 1nd Otsu\n",
    "    binary1 = apply_global_otsu(img_eq)\n",
    "\n",
    "    # Mean filter\n",
    "    binary1_meanfiltered = mean_filter(binary1)\n",
    "\n",
    "    # Scale to 8 bit image (0-255)\n",
    "    binary1_meanfiltered = (binary1_meanfiltered * 255.0 / np.max(binary1_meanfiltered)).astype(np.uint8)\n",
    "\n",
    "    # 2nd Otsu\n",
    "    binary2 = apply_global_otsu(binary1_meanfiltered)\n",
    "\n",
    "    # Invert if necessary\n",
    "    if np.mean(binary2)> .5:            \n",
    "        binary2 = ~binary2\n",
    "\n",
    "    # 5. Compute Dice score\n",
    "    return dice_score(binary2.flatten(), gt.flatten())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Mainroutine: Going through all image-gt pairs and collect all dice scores of data set: NIH3T3\n",
    "# -------------------------------------------------------------------\n",
    "img_dir = Path(\"data/NIH3T3/img\")\n",
    "gt_dir  = Path(\"data/NIH3T3/gt\")\n",
    "\n",
    "dice_scores = []\n",
    "\n",
    "for img_file in sorted(img_dir.glob(\"dna-*.png\")):\n",
    "    # Extract index to read the corresponding gt\n",
    "    idx = img_file.stem.split('-')[-1]\n",
    "    gt_file = gt_dir / f\"{idx}.png\"\n",
    "\n",
    "    if not gt_file.exists():\n",
    "        print(f\" ! Kein Ground-Truth für {img_file.name} ! überspringe …\")\n",
    "        continue\n",
    "\n",
    "    score = process_single(img_file, gt_file)\n",
    "    dice_scores.append(score)\n",
    "\n",
    "# Vector containing all dice scores\n",
    "dice_scores_NIH3T3 = np.array(dice_scores)           \n",
    "print(np.mean(dice_scores_NIH3T3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Mainroutine: Going through all image-gt pairs and collect all dice scores of dataset N2DL-HeLa\n",
    "# -------------------------------------------------------------------\n",
    "img_dir = Path(\"data/N2DL-HeLa/img\")\n",
    "gt_dir  = Path(\"data/N2DL-HeLa/gt\")\n",
    "\n",
    "dice_scores_N2DL_HeLa = []\n",
    "\n",
    "for img_file in sorted(img_dir.glob(\"t-*.tif\")):\n",
    "    # Extract index to read the corresponding gt\n",
    "    idx = img_file.stem.split('-')[-1]\n",
    "    gt_file = gt_dir / f\"man_seg{idx}.tif\"\n",
    "\n",
    "    if not gt_file.exists():\n",
    "        print(f\" ! Kein Ground-Truth für {img_file.name} ! überspringe …\")\n",
    "        continue\n",
    "\n",
    "    scores_N2DL_HeLa = process_single(img_file, gt_file)\n",
    "    dice_scores_N2DL_HeLa.append(scores_N2DL_HeLa)\n",
    "\n",
    "\n",
    "\n",
    "# Vector containing all dice scores\n",
    "dice_scores_N2DL_HeLa = np.array(dice_scores_N2DL_HeLa)           \n",
    "print(np.mean(dice_scores_N2DL_HeLa))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Mainroutine: Going through all image-gt pairs and collect all dice scores of dataset N2DL-HeLa\n",
    "# -------------------------------------------------------------------\n",
    "img_dir = Path(\"data/N2DH-GOWT1/img\")\n",
    "gt_dir  = Path(\"data/N2DH-GOWT1/gt\")\n",
    "\n",
    "dice_scores_N2DH_GOWT1 = []\n",
    "\n",
    "for img_file in sorted(img_dir.glob(\"t-*.tif\")):\n",
    "    # Extract index to read the corresponding gt\n",
    "    idx = img_file.stem.split('-')[-1]\n",
    "    gt_file = gt_dir / f\"man_seg{idx}.tif\"\n",
    "\n",
    "    if not gt_file.exists():\n",
    "        print(f\" ! Kein Ground-Truth für {img_file.name} ! überspringe …\")\n",
    "        continue\n",
    "\n",
    "    scores_N2DH_GOWT1 = process_single(img_file, gt_file)\n",
    "    dice_scores_N2DH_GOWT1.append(scores_N2DH_GOWT1)\n",
    "\n",
    "# Vector containing all dice scores\n",
    "dice_scores_N2DH_GOWT1 = np.array(dice_scores_N2DH_GOWT1)           \n",
    "print(np.mean(dice_scores_N2DH_GOWT1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----\n",
    "#PLOTS\n",
    "#-----\n",
    "\n",
    "# All file names safed in labels\n",
    "labels = [\n",
    "    'dna-0.png',  'dna-1.png',  'dna-26.png', 'dna-27.png',\n",
    "    'dna-28.png', 'dna-29.png', 'dna-30.png', 'dna-31.png',\n",
    "    'dna-32.png', 'dna-33.png', 'dna-37.png', 'dna-40.png',\n",
    "    'dna-42.png', 'dna-44.png', 'dna-45.png', 'dna-46.png',\n",
    "    'dna-47.png', 'dna-49.png', 't01.tif', 't21.tif', 't31.tif', 't39.tif', 't52.tif', 't72.tif',\n",
    "    't13.tif', 't52.tif', 't75.tif', 't79.tif'\n",
    "]\n",
    "\n",
    "# Define indices for the labels of the later x-axis \n",
    "file_names = np.arange(len(labels))\n",
    "\n",
    "# Load all Otsu dice scores without preprocessing\n",
    "all_dice_scores_otsu = np.load('Dice_score_vectors/otsu_only/all_dice_scores_otsu.npy')\n",
    "\n",
    "# Combining all processed dice scores\n",
    "all_dice_scores_processed = np.concatenate((dice_scores_NIH3T3, dice_scores_N2DL_HeLa, dice_scores_N2DH_GOWT1))\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(file_names, all_dice_scores_otsu, color='C0', label='not processed')\n",
    "plt.scatter(file_names, all_dice_scores_processed, color='C1', label='Processed')\n",
    "\n",
    "# Draw connecting line between corresponding datapoints of different methods\n",
    "for xi, yi1, yi2 in zip(file_names, all_dice_scores_otsu, all_dice_scores_processed):\n",
    "    plt.plot([xi, xi], [yi1, yi2], color='gray', alpha=0.5)\n",
    "\n",
    "# Labels for file names\n",
    "plt.xticks(file_names, labels, rotation=45, ha='right')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('all images')\n",
    "plt.ylabel('Dice scores')\n",
    "plt.title('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1247c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT HISTOGRAM EQUALIZATION\n",
    "\n",
    "# Process single image and its groundtruth\n",
    "def process_single(img_path: Path, gt_path: Path) -> float:\n",
    "    \"\"\" \n",
    "    Reads one image and corresponding groundtruth, proccesses, segments and computes dice score for one image.\n",
    "    \"\"\"\n",
    "    # Reads image and reads, binarizes groundtruth\n",
    "    img = imread(img_path, as_gray=True)\n",
    "    img_scaled  = (img / img.max() * 255).astype(np.uint8)\n",
    "    gt = imread(gt_path, as_gray=True)\n",
    "    gt  = 1 - ((gt / gt.max() * 255).astype(np.uint8) == 0)         \n",
    "\n",
    "    # Mean filter\n",
    "    img_meanfiltered = mean_filter(img_scaled)\n",
    "\n",
    "    # Wiener filter, background estimation and removal\n",
    "    bg  = local_wiener_filter(img_meanfiltered, window_size=10)\n",
    "    img_filtered = img_scaled - bg\n",
    "\n",
    "    # Gamma transformation\n",
    "    img_gamma = gammatransformation(img_filtered)\n",
    "\n",
    "    # 1nd Otsu-thresholding + Meanfilter + 2nd Otsu thresholding \n",
    "    \n",
    "    # 1nd Otsu\n",
    "    binary1 = apply_global_otsu(img_gamma)\n",
    "\n",
    "    # Mean filter\n",
    "    binary1_meanfiltered = mean_filter(binary1, kernel_size=7)\n",
    "\n",
    "    # Scale to 8 bit image (0-255)\n",
    "    binary1_meanfiltered = (binary1_meanfiltered * 255.0 / np.max(binary1_meanfiltered)).astype(np.uint8)\n",
    "\n",
    "    # 2nd Otsu\n",
    "    binary2 = apply_global_otsu(binary1_meanfiltered)\n",
    "\n",
    "    # Invert if necessary\n",
    "    if mode_of_image(binary2) > .5:            \n",
    "        binary2 = 1 - binary2\n",
    "\n",
    "    # 5. Compute Dice score\n",
    "    return dice_score(binary2.flatten(), gt.flatten())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Mainroutine: Going through all image-gt pairs and collect all dice scores of data set: NIH3T3\n",
    "# -------------------------------------------------------------------\n",
    "img_dir = Path(\"data/NIH3T3/img\")\n",
    "gt_dir  = Path(\"data/NIH3T3/gt\")\n",
    "\n",
    "dice_scores = []\n",
    "\n",
    "for img_file in sorted(img_dir.glob(\"dna-*.png\")):\n",
    "    # Extract index to read the corresponding gt\n",
    "    idx = img_file.stem.split('-')[-1]\n",
    "    gt_file = gt_dir / f\"{idx}.png\"\n",
    "\n",
    "    if not gt_file.exists():\n",
    "        print(f\" ! Kein Ground-Truth für {img_file.name} ! überspringe …\")\n",
    "        continue\n",
    "\n",
    "    score = process_single(img_file, gt_file)\n",
    "    dice_scores.append(score)\n",
    "\n",
    "# Vector containing all dice scores\n",
    "dice_scores_NIH3T3 = np.array(dice_scores)           \n",
    "print(np.mean(dice_scores_NIH3T3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Mainroutine: Going through all image-gt pairs and collect all dice scores of dataset N2DL-HeLa\n",
    "# -------------------------------------------------------------------\n",
    "img_dir = Path(\"data/N2DL-HeLa/img\")\n",
    "gt_dir  = Path(\"data/N2DL-HeLa/gt\")\n",
    "\n",
    "dice_scores_N2DL_HeLa = []\n",
    "\n",
    "for img_file in sorted(img_dir.glob(\"t-*.tif\")):\n",
    "    # Extract index to read the corresponding gt\n",
    "    idx = img_file.stem.split('-')[-1]\n",
    "    gt_file = gt_dir / f\"man_seg{idx}.tif\"\n",
    "\n",
    "    if not gt_file.exists():\n",
    "        print(f\" ! Kein Ground-Truth für {img_file.name} ! überspringe …\")\n",
    "        continue\n",
    "\n",
    "    scores_N2DL_HeLa = process_single(img_file, gt_file)\n",
    "    dice_scores_N2DL_HeLa.append(scores_N2DL_HeLa)\n",
    "\n",
    "\n",
    "\n",
    "# Vector containing all dice scores\n",
    "dice_scores_N2DL_HeLa = np.array(dice_scores_N2DL_HeLa)           \n",
    "print(np.mean(dice_scores_N2DL_HeLa))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Mainroutine: Going through all image-gt pairs and collect all dice scores of dataset N2DL-HeLa\n",
    "# -------------------------------------------------------------------\n",
    "img_dir = Path(\"data/N2DH-GOWT1/img\")\n",
    "gt_dir  = Path(\"data/N2DH-GOWT1/gt\")\n",
    "\n",
    "dice_scores_N2DH_GOWT1 = []\n",
    "\n",
    "for img_file in sorted(img_dir.glob(\"t-*.tif\")):\n",
    "    # Extract index to read the corresponding gt\n",
    "    idx = img_file.stem.split('-')[-1]\n",
    "    gt_file = gt_dir / f\"man_seg{idx}.tif\"\n",
    "\n",
    "    if not gt_file.exists():\n",
    "        print(f\" ! Kein Ground-Truth für {img_file.name} ! überspringe …\")\n",
    "        continue\n",
    "\n",
    "    scores_N2DH_GOWT1 = process_single(img_file, gt_file)\n",
    "    dice_scores_N2DH_GOWT1.append(scores_N2DH_GOWT1)\n",
    "\n",
    "# Vector containing all dice scores\n",
    "dice_scores_N2DH_GOWT1 = np.array(dice_scores_N2DH_GOWT1)           \n",
    "print(np.mean(dice_scores_N2DH_GOWT1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-----\n",
    "#PLOTS\n",
    "#-----\n",
    "\n",
    "# All file names safed in labels\n",
    "labels = [\n",
    "    'dna-0.png',  'dna-1.png',  'dna-26.png', 'dna-27.png',\n",
    "    'dna-28.png', 'dna-29.png', 'dna-30.png', 'dna-31.png',\n",
    "    'dna-32.png', 'dna-33.png', 'dna-37.png', 'dna-40.png',\n",
    "    'dna-42.png', 'dna-44.png', 'dna-45.png', 'dna-46.png',\n",
    "    'dna-47.png', 'dna-49.png', 't01.tif', 't21.tif', 't31.tif', 't39.tif', 't52.tif', 't72.tif',\n",
    "    't13.tif', 't52.tif', 't75.tif', 't79.tif'\n",
    "]\n",
    "\n",
    "# Define indices for the labels of the later x-axis \n",
    "file_names = np.arange(len(labels))\n",
    "\n",
    "# Load all Otsu dice scores without preprocessing\n",
    "all_dice_scores_otsu = np.load('Dice_score_vectors/otsu_only/all_dice_scores_otsu.npy')\n",
    "\n",
    "# Combining all processed dice scores\n",
    "all_dice_scores_processed = np.concatenate((dice_scores_NIH3T3, dice_scores_N2DL_HeLa, dice_scores_N2DH_GOWT1))\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(file_names, all_dice_scores_otsu, color='C0', label='not processed')\n",
    "plt.scatter(file_names, all_dice_scores_processed, color='C1', label='Processed')\n",
    "\n",
    "# Draw connecting line between corresponding datapoints of different methods\n",
    "for xi, yi1, yi2 in zip(file_names, all_dice_scores_otsu, all_dice_scores_processed):\n",
    "    plt.plot([xi, xi], [yi1, yi2], color='gray', alpha=0.5)\n",
    "\n",
    "# Labels for file names\n",
    "plt.xticks(file_names, labels, rotation=45, ha='right')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('all images')\n",
    "plt.ylabel('Dice scores')\n",
    "plt.title('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
